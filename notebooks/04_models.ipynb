{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a621e8",
   "metadata": {},
   "source": [
    "\n",
    "# 04 — Model Comparison: Baselines vs XGB and STL+XGB\n",
    "\n",
    "**Project:** Sea Surface Temperature Anomaly Forecasting (SSTA)  \n",
    "**Notebook:** 04_models.ipynb  \n",
    "**Purpose:** Concise, academically styled comparison of multiple models using rolling-origin artifacts.\n",
    "\n",
    "## Scope\n",
    "- Load **metrics** and **predictions** from:\n",
    "  - Baselines (`metrics_<project>.csv`, `preds_<project>.parquet`)\n",
    "  - XGB direct (`metrics_xgb_<project>.csv`, `preds_xgb_<project>.parquet`)\n",
    "  - STL+XGB hybrid (`metrics_stl_xgb_<project>.csv`, `preds_stl_xgb_<project>.parquet`)\n",
    "- Build horizon-wise **leaderboards** (MAE, RMSE, sMAPE, MASE).\n",
    "- Plot **MAE by model per horizon** and **forecast vs. actuals** overlays.\n",
    "- Provide interpretation prompts for reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3d9c0",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup & configuration\n",
    "\n",
    "Point to your processed artifacts directory and project name.  \n",
    "The loader will attempt to auto-detect reasonable file name variants if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "PROJECT = \"plymouth\"     # <- change if your files use a different project name\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_metrics_path(stem: str, project: str) -> Path:\n",
    "    cand = Path(PROCESSED_DIR) / f\"{stem}_{project}.csv\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    # fallback: latest matching file for the stem\n",
    "    matches = sorted(Path(PROCESSED_DIR).glob(f\"{stem}_*.csv\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No {stem}_<project>.csv found in {PROCESSED_DIR}\")\n",
    "    return matches[-1]\n",
    "\n",
    "def detect_preds_path(stem: str, project: str) -> Path:\n",
    "    cand = Path(PROCESSED_DIR) / f\"{stem}_{project}.parquet\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    # fallback: latest matching file for the stem\n",
    "    matches = sorted(Path(PROCESSED_DIR).glob(f\"{stem}_*.parquet\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No {stem}_<project>.parquet found in {PROCESSED_DIR}\")\n",
    "    return matches[-1]\n",
    "\n",
    "# Locate files\n",
    "m_base = detect_metrics_path(\"metrics\", PROJECT)\n",
    "p_base = detect_preds_path(\"preds\", PROJECT)\n",
    "\n",
    "m_xgb  = detect_metrics_path(\"metrics_xgb\", PROJECT)\n",
    "p_xgb  = detect_preds_path(\"preds_xgb\", PROJECT)\n",
    "\n",
    "m_stl  = detect_metrics_path(\"metrics_stl_xgb\", PROJECT)\n",
    "p_stl  = detect_preds_path(\"preds_stl_xgb\", PROJECT)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\" \", m_base.name, p_base.name)\n",
    "print(\" \", m_xgb.name,  p_xgb.name)\n",
    "print(\" \", m_stl.name,  p_stl.name)\n",
    "\n",
    "# Read all\n",
    "metrics_base = pd.read_csv(m_base)\n",
    "metrics_xgb  = pd.read_csv(m_xgb)\n",
    "metrics_stl  = pd.read_csv(m_stl)\n",
    "\n",
    "def load_preds(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    return df.dropna(subset=[\"date\"]).sort_values([\"date\",\"horizon\",\"model\"]).reset_index(drop=True)\n",
    "\n",
    "preds_base = load_preds(p_base)\n",
    "preds_xgb  = load_preds(p_xgb)\n",
    "preds_stl  = load_preds(p_stl)\n",
    "\n",
    "# Normalize model names for neat plots\n",
    "metrics_xgb[\"model\"] = metrics_xgb.get(\"model\", \"xgb_direct\")\n",
    "metrics_stl[\"model\"] = metrics_stl.get(\"model\", \"stl+xgb_resid\")\n",
    "preds_xgb[\"model\"]   = preds_xgb.get(\"model\", \"xgb_direct\")\n",
    "preds_stl[\"model\"]   = preds_stl.get(\"model\", \"stl+xgb_resid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3e014",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Unified leaderboards\n",
    "\n",
    "We concatenate all metrics and produce per-horizon leaderboards (sorted by MAE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb841ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_all = pd.concat([metrics_base, metrics_xgb, metrics_stl], ignore_index=True, sort=False)\n",
    "metrics_all = metrics_all.sort_values([\"horizon\",\"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Unified metrics (top 12 rows):\")\n",
    "print(metrics_all.head(12).to_string(index=False))\n",
    "\n",
    "def leaderboard_for_h(h):\n",
    "    m = metrics_all[metrics_all[\"horizon\"] == h].copy()\n",
    "    return m.sort_values(\"MAE\")[[\"horizon\",\"model\",\"MAE\",\"RMSE\",\"sMAPE\",\"MASE\"]]\n",
    "\n",
    "for h in sorted(metrics_all[\"horizon\"].unique()):\n",
    "    print(\"\\n=== LEADERBOARD: Horizon +%d months ===\" % h)\n",
    "    print(leaderboard_for_h(h).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879aada",
   "metadata": {},
   "source": [
    "\n",
    "## 2. MAE by model per horizon\n",
    "\n",
    "Bar plots of MAE, one panel per horizon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_mae_by_model(h):\n",
    "    m = metrics_all[metrics_all[\"horizon\"] == h].copy()\n",
    "    labels = m[\"model\"].tolist()\n",
    "    vals = m[\"MAE\"].tolist()\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.bar(range(len(labels)), vals)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=20, ha=\"right\")\n",
    "    plt.ylabel(\"MAE (°C)\")\n",
    "    plt.title(f\"MAE by Model — Horizon +{h} months\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "for h in sorted(metrics_all[\"horizon\"].unique()):\n",
    "    plot_mae_by_model(int(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b4123",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Forecast vs. actuals (overlay)\n",
    "\n",
    "We overlay forecasts from the **best model** at each horizon against actuals.  \n",
    "(Uses the predictions files from each model family.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae94162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to get preds df by model keyword\n",
    "def get_preds_for_model(name_keyword: str):\n",
    "    if \"xgb\" in name_keyword and \"stl\" not in name_keyword:\n",
    "        return preds_xgb\n",
    "    if \"stl\" in name_keyword:\n",
    "        return preds_stl\n",
    "    return preds_base\n",
    "\n",
    "def plot_best_overlay(h):\n",
    "    m = leaderboard_for_h(h)\n",
    "    best_model = str(m.iloc[0][\"model\"])\n",
    "    source = get_preds_for_model(best_model)\n",
    "    p = source[source[\"horizon\"] == h].copy()\n",
    "    if p.empty:\n",
    "        print(f\"No predictions for horizon={h} in {best_model}.\")\n",
    "        return\n",
    "    # Actuals (from the selected preds table)\n",
    "    actual = p[[\"date\",\"y_true\"]].drop_duplicates().sort_values(\"date\")\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(actual[\"date\"], actual[\"y_true\"], label=\"Actual SSTA\")\n",
    "    # Forecast\n",
    "    pm = p[p[\"model\"] == best_model].sort_values(\"date\")\n",
    "    plt.plot(pm[\"date\"], pm[\"y_pred\"], label=best_model)\n",
    "    plt.title(f\"Best Model vs Actuals — Horizon +{h} months\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Anomaly (°C)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "for h in sorted(metrics_all[\"horizon\"].unique()):\n",
    "    plot_best_overlay(int(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e5b35",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Relative improvement\n",
    "\n",
    "Compare each model’s MAE to the **seasonal naïve** baseline at the same horizon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def improvement_vs_seasonal_naive(h):\n",
    "    m = metrics_all[metrics_all[\"horizon\"] == h].copy()\n",
    "    base = m[m[\"model\"].str.contains(\"seasonal_naive\")][\"MAE\"]\n",
    "    if base.empty:\n",
    "        print(f\"Seasonal naïve not found for horizon {h}.\")\n",
    "        return\n",
    "    b = float(base.iloc[0])\n",
    "    m = m.assign(Improvement_vs_Seasonal_Naive_pct = 100.0 * (1.0 - m[\"MAE\"] / b))\n",
    "    print(m[[\"model\",\"MAE\",\"Improvement_vs_Seasonal_Naive_pct\"]].sort_values(\"MAE\").to_string(index=False))\n",
    "\n",
    "for h in sorted(metrics_all[\"horizon\"].unique()):\n",
    "    print(f\"\\n=== Improvement vs Seasonal Naïve — h=+{h} ===\")\n",
    "    improvement_vs_seasonal_naive(int(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f87e7",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Interpretation prompts\n",
    "- **Winners by horizon:** Which models dominate MAE at +1, +3, +6? Are the gains consistent?\n",
    "- **Practical significance:** Are MAE improvements (°C) large relative to typical SSTA variability?\n",
    "- **Robustness:** Do results hold across subperiods (e.g., 1982–2000 vs 2001–present)?\n",
    "- **Complexity vs. gain:** Do advanced models materially outperform seasonal naïve and SARIMA?\n",
    "- **Next steps:** Tune hyperparameters, add climate indices with delays, or test multi-horizon direct models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
